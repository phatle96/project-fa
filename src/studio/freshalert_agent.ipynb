{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b8ad316",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph-checkpoint-sqlite langchain_core langgraph langchain-groq langchain-mcp-adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "555bf6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.11 (main, Feb 12 2025, 14:51:05) [Clang 19.1.6 ]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0316b05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tools = None\n",
    "_model = None\n",
    "_memory = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c52ad05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4e985f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiosqlite\n",
    "db_path = \"notebook_data/database.db\"\n",
    "conn = await aiosqlite.connect(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53f90e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is our checkpointer \n",
    "\n",
    "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
    "\n",
    "# Create an async version of the memory object\n",
    "memory = AsyncSqliteSaver(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f932d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phatle/projects/work/fresh-alert-agent/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import TypedDict, List, Callable, Optional, Dict, Any\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ModelConfig(TypedDict):\n",
    "    provider: str\n",
    "    model_name: str\n",
    "\n",
    "\n",
    "def get_model(model_config: ModelConfig = None):\n",
    "    \"\"\"\n",
    "    Get the appropriate LLM model based on configuration.\n",
    "\n",
    "    Args:\n",
    "        model_config: Optional model configuration with provider and model_name\n",
    "\n",
    "    Returns:\n",
    "        Configured LLM instance\n",
    "    \"\"\"\n",
    "\n",
    "    env = os.getenv(\"ENVIRONMENT\", \"dev\").lower()\n",
    "\n",
    "    # Always create new model based on config (no global caching)\n",
    "    if not model_config:\n",
    "        if env == \"prod\":\n",
    "            model_name = os.getenv(\"ANTHROPIC_MODEL\", \"claude-3-haiku-20240307\")\n",
    "            return ChatAnthropic(model=model_name, temperature=0)\n",
    "        else:\n",
    "            model_name = os.getenv(\"OPENAI_MODEL\", \"gpt-5-mini\")\n",
    "            return ChatOpenAI(model=model_name, temperature=0)\n",
    "\n",
    "    # Create model based on user selection\n",
    "    provider = model_config.get(\"provider\", \"groq\")\n",
    "    model_name = model_config.get(\"model_name\", \"llama-3.1-8b-instant\")\n",
    "\n",
    "    if provider == \"anthropic\":\n",
    "        return ChatAnthropic(model=model_name, temperature=0)\n",
    "\n",
    "    if provider == \"openai\":\n",
    "        return ChatOpenAI(model=model_name, temperature=0)\n",
    "\n",
    "    return ChatGroq(model=model_name, temperature=0)\n",
    "\n",
    "\n",
    "async def get_tools(config: Optional[RunnableConfig] = None) -> List:\n",
    "    \"\"\"\n",
    "    Get all available tools for the agent with dynamic authentication.\n",
    "\n",
    "    Args:\n",
    "        config: RunnableConfig containing authentication tokens\n",
    "\n",
    "    Returns:\n",
    "        List of available tools\n",
    "    \"\"\"\n",
    "\n",
    "    fresh_alert_mcp_url = os.getenv(\"FRESH_ALERT_MCP\", \"\")\n",
    "    spoonacular_mcp_url = os.getenv(\"SPOONACULAR_MCP\", \"\")\n",
    "\n",
    "    fresh_alert_token = \"\"\n",
    "    headers = {}\n",
    "    if config and \"configurable\" in config:\n",
    "        user_config = config[\"configurable\"].get(\"langgraph_auth_user\", {})\n",
    "        fresh_alert_token = user_config.get(\"freshalert-token\", \"\")\n",
    "        \n",
    "        headers = {\"authorization\": f\"Bearer {fresh_alert_token}\"}\n",
    "        \n",
    "    print(\"-> fresh_alert_token: \", fresh_alert_token)\n",
    "        \n",
    "\n",
    "    client = MultiServerMCPClient(\n",
    "        {\n",
    "            \"fresh_alert_mcp\": {\n",
    "                \"transport\": \"streamable_http\",\n",
    "                \"url\": fresh_alert_mcp_url,\n",
    "                \"headers\": headers,\n",
    "            },\n",
    "            \"spoonacular_mcp\": {\n",
    "                \"url\": spoonacular_mcp_url,\n",
    "                \"transport\": \"streamable_http\",\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return await client.get_tools()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
